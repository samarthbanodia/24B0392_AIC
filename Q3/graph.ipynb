{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Chatbot with KV Caching ,History awareness , Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Librarries and Packages\n",
    "\n",
    "Recrusive character text splitter - Chunking text\n",
    "\n",
    "OpenAi(embeddings) , Deepseek(answer generation) models\n",
    "\n",
    "FAISS - vector db\n",
    "\n",
    "Prompt Template for formating prompt\n",
    "\n",
    "Pdf reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Document Text function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_content(documents):\n",
    "    raw_text = \"\"\n",
    "\n",
    "    for document in documents:\n",
    "        pdf_reader = PdfReader(document)\n",
    "        for page in pdf_reader.pages:\n",
    "            raw_text += page.extract_text()\n",
    "\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking text using RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(text):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = splitter.create_documents([text])\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Embeddings using openai embedding-3 model and store in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_openai(chunks):\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        api_key = 'api_key',\n",
    "        model=\"text-embedding-3-small\")\n",
    "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve Top-k(=4) queries using similarity search on vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_queries(vector_store):\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising Deepseek LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatDeepSeek(\n",
    "    api_key='api_key',\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Template for the LLM with History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "      You are a helpful assistant.\n",
    "      Answer ONLY from understandingthe provided transcript context and the conversation history.\n",
    "      If the context and history are insufficient, just say you don't know.\n",
    "\n",
    "      {context}\n",
    "\n",
    "      history : {history}\n",
    "\n",
    "      Question: {question}\n",
    "    \"\"\",\n",
    "    input_variables = ['context', 'history' ,'question']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History Compiling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_history(history):\n",
    "    return \"\\n\".join(f\"{msg['role']}: {msg['content']}\" for msg in history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entities and Relation extraction function using Prompt \n",
    "\n",
    "Return Entites , relations in Cypher format for neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_relation(doc_text):\n",
    "\n",
    "     summary_prompt = PromptTemplate(\n",
    "     template = '''Summarize the following text by extracting only the key entities, their types, and relationships relevant for building a knowledge graph. Provide the summary as a list of entities and their connections in a clear, concise format without any extra explanation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "           document_text = {doc_text}\n",
    "        ''',\n",
    "\n",
    "      input_variables = ['doc_text']  )\n",
    "\n",
    "     summary = summary_prompt.invoke({\"doc_text\" : doc_text})\n",
    "     prompt = PromptTemplate(\n",
    "     template = '''Generate Cypher queries to create a knowledge graph from the following text.\n",
    "                  **Return only the Cypher code as plain text with no markdown formatting, no comments, and no extra explanation.**\n",
    "                  Do not include any backticks (```) or other formatting characters.\n",
    "\n",
    "           summary = {summary}\n",
    "        ''',\n",
    "\n",
    "      input_variables = ['summary']  )\n",
    "\n",
    "     final_prompt = prompt.invoke({\"summary\":summary})\n",
    "     result = llm.invoke(final_prompt)\n",
    "     return result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare session variables and import neo4j libariries \n",
    "\n",
    "IFrame for saving the graph.html locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'history' not in st.session_state:\n",
    "    st.session_state.history = []\n",
    "\n",
    "if 'kv_cache' not in st.session_state:\n",
    "    st.session_state.kv_cache = {}\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from pyvis.network import Network\n",
    "from IPython.display import IFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up Neo4j and graph function\n",
    "\n",
    "This function saves the graph locally in a html file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\"neo4j+s://a80e37c4.databases.neo4j.io\", auth=(\"neo4j\", \"epgoSPMud2qlmFJEzS9Q5amNMGpz1EI4F1YCftHVawM\"))\n",
    "\n",
    "def display_graph(cypher,html_path=\"graph.html\"):\n",
    "    with driver.session() as session:\n",
    "         session.run(cypher)\n",
    "\n",
    "    # Build and show graph\n",
    "    net = Network(notebook=True)\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"MATCH (a)-[r]->(b) RETURN a, r, b\")\n",
    "        for record in result:\n",
    "            a = record[\"a\"]\n",
    "            b = record[\"b\"]\n",
    "            r = record[\"r\"]\n",
    "            net.add_node(a.id, label=a.get(\"name\", str(a.id)))\n",
    "            net.add_node(b.id, label=b.get(\"name\", str(b.id)))\n",
    "            net.add_edge(a.id, b.id, label=r.type)\n",
    "\n",
    "    net.show(\"graph.html\")\n",
    "    return IFrame(src=html_path, width=\"100%\", height=\"650px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streamlit workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title(\"PDF Chatbot\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Choose a PDF file\", type=\"pdf\")\n",
    "if uploaded_file is not None:\n",
    "    text = get_pdf_content([uploaded_file]) #get text\n",
    "    cypher = str(extract_entities_relation(text)) #create cyphers for the summary\n",
    "\n",
    "    print(cypher)\n",
    "\n",
    "    display_graph(cypher) #saved the graph.html locally for viewing\n",
    "\n",
    "    split_text = get_chunks(text) #splits chunks\n",
    "    st.write(\"PDF text split in to chunks.\")\n",
    "    embedding_index = get_embeddings_openai(split_text,text)\n",
    "    similar_chunks = top_k_queries(embedding_index)\n",
    "\n",
    "    st.write(\"embedding peformed on chunks.\")\n",
    "    st.write(\"PDF text extracted. You can now ask questions.\")\n",
    "\n",
    "    user_question = st.text_input(\"Ask a question about the PDF:\")\n",
    "    if user_question:\n",
    "        key_kv_cache = user_question.strip().lower()  #strips and lowercases the key\n",
    "\n",
    "        if key_kv_cache in st.session_state.kv_cache:  #checking if key alreadu exists\n",
    "            answer = st.session_state.kv_cache[key_kv_cache] #if yes return without invovling LLMS\n",
    "            # print(\"in cache in cache in cache\")\n",
    "\n",
    "\n",
    "        else: #else create new key\n",
    "            retrieved_docs = similar_chunks.invoke(user_question)\n",
    "            context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "            st.session_state.history.append({\"role\": \"user\", \"content\": user_question}) #append the history dictionary after evey conversation\n",
    "\n",
    "            history_string = compile_history(st.session_state.history)\n",
    "\n",
    "            final_prompt = prompt.invoke({\n",
    "                 \"context\": context_text,\n",
    "                 \"history\": history_string,\n",
    "                 \"question\": user_question\n",
    "             })\n",
    "\n",
    "            answer = llm.invoke(final_prompt) #store answer\n",
    "            st.session_state.kv_cache[key_kv_cache] = answer\n",
    "            st.session_state.history.append({\"role\": \"Chatbot\", \"content\": str(answer.content)})\n",
    "            print(\"yeah\")\n",
    "\n",
    "             # Display the answer\n",
    "        st.write(f\"Answer: {answer.content}\")\n",
    "       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
